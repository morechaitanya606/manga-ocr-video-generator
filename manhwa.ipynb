{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7BIvyUFHs79NUAR0bPnjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morechaitanya606/manga-ocr-video-generator/blob/main/manhwa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxR9ytnJSDOK",
        "outputId": "2f81059a-d862-40c8-9b33-857e2f328fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.12/dist-packages (2.37.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy==1.0.3) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy==1.0.3) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy==1.0.3) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy==1.0.3) (2.0.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy==1.0.3) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio[ffmpeg]) (11.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from imageio[ffmpeg]) (5.9.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2025.8.3)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.1)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from gTTS) (2.32.4)\n",
            "Collecting click<8.2,>=7.1 (from gTTS)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->gTTS) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.8.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, pymupdf, ninja, click, gTTS, easyocr\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "Successfully installed click-8.1.8 easyocr-1.7.2 gTTS-2.5.4 ninja-1.13.0 pyclipper-1.3.0.post6 pymupdf-1.26.4 python-bidi-0.6.6\n"
          ]
        }
      ],
      "source": [
        "!pip install moviepy==1.0.3 imageio[ffmpeg]\n",
        "!pip install pymupdf easyocr gTTS transformers torch\n",
        "!pip install -q edge-tts asyncio nest-asyncio\n",
        "!pip install -q opencv-python numpy scikit-image\n",
        "!pip install -q sentence-transformers accelerate pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "uXtlp9pfk0O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "AI Manga Recap Test Script (Colab-Optimized)\n",
        "- Extracts pages from a PDF\n",
        "- Uses OCR to get text\n",
        "- AI model generates Hindi narration\n",
        "- TTS + video generation\n",
        "\"\"\"\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Install dependencies\n",
        "# ================================\n",
        "#!pip install moviepy pillow easyocr PyMuPDF gTTS transformers\n",
        "\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import easyocr\n",
        "from gtts import gTTS\n",
        "import moviepy.editor as mp\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from PIL import Image\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "PDF_PATH = \"/content/Ch_199_Side_Story_20.pdf\"   # Upload your PDF in Colab\n",
        "OUTPUT_DIR = \"output\"\n",
        "CHAPTER = 1\n",
        "MODEL_NAME = \"google/flan-t5-base\"  # Lightweight AI model\n",
        "# ----------------------------\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ================================\n",
        "# 1. Extract first few pages as images\n",
        "# ================================\n",
        "def extract_pdf_images(pdf_path, max_pages=2):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    images = []\n",
        "    for i in range(min(len(doc), max_pages)):\n",
        "        page = doc.load_page(i)\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))\n",
        "        img_path = os.path.join(OUTPUT_DIR, f\"page_{i+1}.jpg\")\n",
        "        pix.save(img_path)\n",
        "        images.append(img_path)\n",
        "    doc.close()\n",
        "    return images\n",
        "\n",
        "# ================================\n",
        "# 2. OCR text from pages\n",
        "# ================================\n",
        "def ocr_text(images):\n",
        "    reader = easyocr.Reader(['en', 'hi'])  # English + Hindi OCR\n",
        "    all_text = []\n",
        "    for img in images:\n",
        "        result = reader.readtext(img, detail=0)\n",
        "        all_text.append(\" \".join(result))\n",
        "    return \" \".join(all_text)\n",
        "\n",
        "# ================================\n",
        "# 3. Use AI model to generate Hindi recap\n",
        "# ================================\n",
        "# ================================\n",
        "# 3. Use AI model to generate Hindi recap (improved)\n",
        "# ================================\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# 3. Generate clean Hindi script with translation + summarization\n",
        "def generate_hindi_script(english_text):\n",
        "    # --- Translation (EN -> HI) ---\n",
        "    model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "    translated = model.generate(\n",
        "        **tokenizer(english_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    )\n",
        "    hindi_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "    # --- Summarization in Hindi ---\n",
        "    summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
        "    summary = summarizer(\n",
        "        hindi_text,\n",
        "        max_length=80,\n",
        "        min_length=30,\n",
        "        do_sample=False\n",
        "    )[0][\"summary_text\"]\n",
        "\n",
        "    # --- Clean output ---\n",
        "    summary = summary.replace(\"\\n\", \" \").strip()\n",
        "    if not summary:\n",
        "        summary = \"‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§¨‡§π‡•Å‡§§ ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡§§‡•Ä ‡§π‡•à‡•§\"\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4. Convert to speech\n",
        "# ================================\n",
        "def create_audio(text, out_file):\n",
        "    if not text.strip():\n",
        "        text = \"‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§π‡•à‡•§\"  # default safe text\n",
        "    tts = gTTS(text=text, lang=\"hi\")\n",
        "    tts.save(out_file)\n",
        "    return out_file\n",
        "\n",
        "# ================================\n",
        "# 5. Resize oversized images\n",
        "# ================================\n",
        "def resize_image(input_path, max_width=1080, max_height=1920):\n",
        "    img = Image.open(input_path)\n",
        "    w, h = img.size\n",
        "    if w > max_width or h > max_height:\n",
        "        img.thumbnail((max_width, max_height), Image.Resampling.LANCZOS)\n",
        "        img.save(input_path)\n",
        "    return input_path\n",
        "\n",
        "# ================================\n",
        "# 6. Make video\n",
        "# ================================\n",
        "# 5. Make video in 16:9 format\n",
        "# 5. Make video in 16:9 format\n",
        "def make_video(images, audio_file, out_file=\"test_video.mp4\"):\n",
        "    audio_clip = mp.AudioFileClip(audio_file)\n",
        "    duration = audio_clip.duration / len(images)\n",
        "\n",
        "    clips = []\n",
        "    for img in images:\n",
        "        clip = mp.ImageClip(img)\n",
        "\n",
        "        # Scale to fit within 1920x1080\n",
        "        if clip.w > clip.h:\n",
        "            clip = clip.resize(width=1920)\n",
        "        else:\n",
        "            clip = clip.resize(height=1080)\n",
        "\n",
        "        # Pad to exactly 1920x1080 (letterbox)\n",
        "        clip = clip.on_color(\n",
        "            size=(1920, 1080),\n",
        "            color=(0, 0, 0),  # black background\n",
        "            pos=(\"center\", \"center\")\n",
        "        )\n",
        "\n",
        "        clip = clip.set_duration(duration)\n",
        "        clips.append(clip)\n",
        "\n",
        "    video = mp.concatenate_videoclips(clips, method=\"compose\")\n",
        "    final = video.set_audio(audio_clip)\n",
        "    final.write_videofile(\n",
        "        out_file,\n",
        "        fps=30,\n",
        "        codec=\"libx264\",\n",
        "        audio_codec=\"aac\",\n",
        "        threads=4\n",
        "    )\n",
        "    return out_file\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# üöÄ MAIN\n",
        "# ================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üìñ Extracting images...\")\n",
        "    imgs = extract_pdf_images(PDF_PATH, max_pages=2)\n",
        "\n",
        "    print(\"üîé Running OCR...\")\n",
        "    raw_text = ocr_text(imgs)\n",
        "\n",
        "    print(\"ü§ñ Generating Hindi script...\")\n",
        "    script_text = generate_hindi_script(raw_text)\n",
        "    print(\"Generated Script:\", script_text)\n",
        "\n",
        "    print(\"üéôÔ∏è Creating audio...\")\n",
        "    audio = create_audio(script_text, os.path.join(OUTPUT_DIR, \"chapter_audio.mp3\"))\n",
        "\n",
        "    print(\"üé¨ Making video...\")\n",
        "    video_path = make_video(imgs, audio, os.path.join(OUTPUT_DIR, f\"chapter_{CHAPTER}.mp4\"))\n",
        "\n",
        "    print(f\"‚úÖ Done! Video saved at: {video_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2LqKLrFaNR5",
        "outputId": "07124571-ad1f-461e-b9e9-e8a52ca3f5ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Extracting images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Running OCR...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Generating Hindi script...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "\n",
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Your max_length is set to 80, but your input_length is only 25. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Script: ‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§¨‡§π‡•Å‡§§ ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§π‡•à ‡§î‡§∞ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡§§‡•Ä ‡§π‡•à‡•§\n",
            "üéôÔ∏è Creating audio...\n",
            "üé¨ Making video...\n",
            "Moviepy - Building video output/chapter_1.mp4.\n",
            "MoviePy - Writing audio in chapter_1TEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video output/chapter_1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready output/chapter_1.mp4\n",
            "‚úÖ Done! Video saved at: output/chapter_1.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1936db4a",
        "outputId": "e003a33f-f56a-41cb-9051-2ec1c51bf749"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced AI Manga Recap Generator (Colab-Optimized for T4 GPU)\n",
        "- Extracts high-quality pages from PDF\n",
        "- Advanced OCR with noise reduction\n",
        "- Better AI models for content generation\n",
        "- High-quality TTS with multiple voices\n",
        "- Professional video generation with effects\n",
        "\"\"\"\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Install dependencies (run this cell first)\n",
        "# ================================\n",
        "\"\"\"\n",
        "!pip install -q moviepy pillow easyocr PyMuPDF transformers torch accelerate\n",
        "!pip install -q edge-tts asyncio nest-asyncio\n",
        "!pip install -q opencv-python numpy scikit-image\n",
        "!pip install -q sentence-transformers\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import fitz  # PyMuPDF\n",
        "import easyocr\n",
        "import edge_tts\n",
        "import moviepy.editor as mp\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Enable nested asyncio for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ================================\n",
        "# üéØ ENHANCED CONFIG\n",
        "# ================================\n",
        "PDF_PATH = \"/content/Ch_199_Side_Story_20.pdf\"   # Upload your PDF in Colab\n",
        "OUTPUT_DIR = \"output\"\n",
        "CHAPTER = 1\n",
        "\n",
        "# AI Models (optimized for T4)\n",
        "SUMMARIZATION_MODEL = \"facebook/bart-large-cnn\"  # Better summarization\n",
        "TRANSLATION_MODEL = \"Helsinki-NLP/opus-mt-en-hi\"  # English to Hindi\n",
        "\n",
        "# Voice settings (Edge TTS - high quality, free)\n",
        "VOICE_OPTIONS = [\n",
        "    \"hi-IN-MadhurNeural\",     # Male, clear\n",
        "    \"hi-IN-SwaraNeural\",      # Female, expressive\n",
        "    \"hi-IN-AnanyaNeural\"      # Female, warm\n",
        "]\n",
        "SELECTED_VOICE = VOICE_OPTIONS[0]  # Change index for different voice\n",
        "\n",
        "# Video settings\n",
        "VIDEO_WIDTH = 1920\n",
        "VIDEO_HEIGHT = 1080\n",
        "FPS = 30\n",
        "BITRATE = \"8000k\"  # High quality for YouTube\n",
        "\n",
        "# ================================\n",
        "# üîß UTILITY FUNCTIONS\n",
        "# ================================\n",
        "def setup_environment():\n",
        "    \"\"\"Setup GPU and create directories\"\"\"\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"üöÄ Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    \"\"\"Enhance image quality for better OCR\"\"\"\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Denoise\n",
        "    denoised = cv2.fastNlMeansDenoising(gray)\n",
        "\n",
        "    # Enhance contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    enhanced = clahe.apply(denoised)\n",
        "\n",
        "    # Sharpen\n",
        "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
        "    sharpened = cv2.filter2D(enhanced, -1, kernel)\n",
        "\n",
        "    # Save enhanced image\n",
        "    enhanced_path = img_path.replace('.jpg', '_enhanced.jpg')\n",
        "    cv2.imwrite(enhanced_path, sharpened)\n",
        "    return enhanced_path\n",
        "\n",
        "# ================================\n",
        "# üìñ PDF EXTRACTION (ENHANCED)\n",
        "from PIL import Image\n",
        "Image.MAX_IMAGE_PIXELS = None  # disable DecompressionBombWarning\n",
        "\n",
        "# ================================\n",
        "def extract_pdf_images(pdf_path, max_pages=5):\n",
        "    \"\"\"Extract high-quality images from PDF (skip if already extracted).\"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "\n",
        "    images = []\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    for i in range(min(len(doc), max_pages)):\n",
        "        img_path = os.path.join(OUTPUT_DIR, f\"page_{i+1}.jpg\")\n",
        "        enhanced_path = img_path.replace(\".jpg\", \"_enhanced.jpg\")\n",
        "\n",
        "        if os.path.exists(enhanced_path):\n",
        "            print(f\"‚è© Skipped extraction, using cached: page {i+1}\")\n",
        "            images.append(enhanced_path)\n",
        "            continue\n",
        "\n",
        "        # Reduce zoom to avoid gigantic images\n",
        "        mat = fitz.Matrix(2.0, 2.0)\n",
        "        pix = doc.load_page(i).get_pixmap(matrix=mat, alpha=False)\n",
        "        pix.save(img_path)\n",
        "\n",
        "        # Auto-resize if image is still too large\n",
        "        img = cv2.imread(img_path)\n",
        "        h, w = img.shape[:2]\n",
        "        max_dim = 4000  # keep under 4k px per side\n",
        "        if max(h, w) > max_dim:\n",
        "            scale = max_dim / max(h, w)\n",
        "            resized = cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
        "            cv2.imwrite(img_path, resized)\n",
        "\n",
        "        enhanced_path = preprocess_image(img_path)\n",
        "        images.append(enhanced_path)\n",
        "        print(f\"‚úÖ Extracted page {i+1} ({w}x{h} ‚Üí resized if needed)\")\n",
        "\n",
        "    doc.close()\n",
        "    return images\n",
        "\n",
        "\n",
        "# ================================\n",
        "# üîç ADVANCED OCR\n",
        "# ================================\n",
        "def advanced_ocr(images):\n",
        "    \"\"\"Perform OCR with EasyOCR (auto downscale large images).\"\"\"\n",
        "    reader = easyocr.Reader([\"en\"], gpu=True)\n",
        "    all_text = []\n",
        "\n",
        "    for idx, img_path in enumerate(images, 1):\n",
        "        print(f\"üîé Processing OCR for page {idx}...\")\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # ‚úÖ Safety resize if image too big for OpenCV warp\n",
        "        h, w = img.shape[:2]\n",
        "        if h > 3000 or w > 3000:\n",
        "            scale = 2000 / max(h, w)\n",
        "            img = cv2.resize(img, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
        "            print(f\"‚ö†Ô∏è Resized page {idx} for OCR: {w}x{h} ‚Üí {img.shape[1]}x{img.shape[0]}\")\n",
        "\n",
        "        try:\n",
        "            results = reader.readtext(img, detail=1, paragraph=True)\n",
        "            page_text = \" \".join([res[1] for res in results])\n",
        "            print(f\"üìÑ Page {idx}: {len(page_text)} characters extracted\")\n",
        "            all_text.append(page_text)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OCR failed on page {idx}: {e}\")\n",
        "            all_text.append(\"\")\n",
        "\n",
        "    return \"\\n\".join(all_text)\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# ü§ñ ENHANCED AI CONTENT GENERATION\n",
        "# ================================\n",
        "class ContentGenerator:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        print(\"ü§ñ Loading AI models...\")\n",
        "\n",
        "        # Summarization model\n",
        "        self.summarizer = pipeline(\n",
        "            \"summarization\",\n",
        "            model=SUMMARIZATION_MODEL,\n",
        "            device=0 if device == \"cuda\" else -1,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "        )\n",
        "\n",
        "        # Translation model\n",
        "        self.translator_tokenizer = AutoTokenizer.from_pretrained(TRANSLATION_MODEL)\n",
        "        self.translator_model = AutoModelForSeq2SeqLM.from_pretrained(TRANSLATION_MODEL)\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            self.translator_model = self.translator_model.half().cuda()\n",
        "\n",
        "        print(\"‚úÖ AI models loaded successfully!\")\n",
        "\n",
        "    def generate_engaging_script(self, raw_text):\n",
        "        \"\"\"Generate engaging Hindi narration\"\"\"\n",
        "        if not raw_text or len(raw_text) < 50:\n",
        "            return \"‡§á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§™‡§∏‡§Ç‡§¶ ‡§Ü‡§è‡§ó‡•Ä‡•§\"\n",
        "\n",
        "        try:\n",
        "            # Step 1: Summarize in English (better quality)\n",
        "            print(\"üìù Generating summary...\")\n",
        "\n",
        "            # Split text if too long\n",
        "            max_chunk = 1000\n",
        "            if len(raw_text) > max_chunk:\n",
        "                chunks = [raw_text[i:i+max_chunk] for i in range(0, len(raw_text), max_chunk)]\n",
        "                summaries = []\n",
        "                for chunk in chunks:\n",
        "                    if len(chunk) > 50:\n",
        "                        summary = self.summarizer(\n",
        "                            chunk,\n",
        "                            max_length=100,\n",
        "                            min_length=30,\n",
        "                            do_sample=False\n",
        "                        )[0]['summary_text']\n",
        "                        summaries.append(summary)\n",
        "                english_summary = \" \".join(summaries)\n",
        "            else:\n",
        "                english_summary = self.summarizer(\n",
        "                    raw_text,\n",
        "                    max_length=150,\n",
        "                    min_length=40,\n",
        "                    do_sample=False\n",
        "                )[0]['summary_text']\n",
        "\n",
        "            # Step 2: Enhance for storytelling\n",
        "            storytelling_prompt = f\"\"\"\n",
        "            Transform this manga summary into an engaging Hindi narration for YouTube:\n",
        "            {english_summary}\n",
        "\n",
        "            Make it dramatic and exciting for viewers.\n",
        "            \"\"\"\n",
        "\n",
        "            # Step 3: Translate to Hindi\n",
        "            print(\"üåç Translating to Hindi...\")\n",
        "            inputs = self.translator_tokenizer(\n",
        "                english_summary,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            if self.device == \"cuda\":\n",
        "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.translator_model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=200,\n",
        "                    num_beams=4,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True\n",
        "                )\n",
        "\n",
        "            hindi_text = self.translator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Step 4: Add engaging elements\n",
        "            hindi_text = self.enhance_narration(hindi_text)\n",
        "\n",
        "            # Save generated script\n",
        "            with open(os.path.join(OUTPUT_DIR, \"generated_script.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"English Summary:\\n{english_summary}\\n\\nHindi Script:\\n{hindi_text}\")\n",
        "\n",
        "            return hindi_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è AI generation failed: {e}\")\n",
        "            return \"‡§á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§ò‡§ü‡§®‡§æ‡§è‡§Ç ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§π‡•à‡§∞‡§æ‡§® ‡§ï‡§∞ ‡§¶‡•á‡§Ç‡§ó‡•Ä‡•§ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§®‡§è ‡§Æ‡•ã‡§°‡§º ‡§Ü‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§ï‡§ø‡§∞‡§¶‡§æ‡§∞ ‡§Ö‡§™‡§®‡•Ä ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡§§‡•á ‡§π‡•à‡§Ç‡•§\"\n",
        "\n",
        "    def enhance_narration(self, text):\n",
        "        \"\"\"Add dramatic elements to Hindi narration\"\"\"\n",
        "        # Add pauses and emphasis\n",
        "        text = text.replace(\"‡•§\", \"... \")\n",
        "        text = text.replace(\",\", \"... \")\n",
        "\n",
        "        # Add engaging intro/outro\n",
        "        intros = [\n",
        "            \"‡§Ü‡§ú ‡§ï‡•á ‡§á‡§∏ ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç... \",\n",
        "            \"‡§á‡§∏ ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§Ü‡§ó‡•á... \",\n",
        "            \"‡§Ö‡§¨ ‡§¶‡•á‡§ñ‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à... \"\n",
        "        ]\n",
        "\n",
        "        outros = [\n",
        "            \"... ‡§Ø‡§π ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§®‡•Ä ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à!\",\n",
        "            \"... ‡§Ü‡§ó‡•á ‡§î‡§∞ ‡§≠‡•Ä ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö ‡§ï‡§æ ‡§á‡§Ç‡§§‡§ú‡§æ‡§∞ ‡§π‡•à!\",\n",
        "            \"... ‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§µ‡§æ‡§ï‡§à ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§•‡§æ!\"\n",
        "        ]\n",
        "\n",
        "        enhanced = f\"{intros[0]}{text}{outros[0]}\"\n",
        "        return enhanced\n",
        "\n",
        "# ================================\n",
        "# üéôÔ∏è HIGH-QUALITY TTS\n",
        "# ================================\n",
        "async def create_high_quality_audio(text, output_path):\n",
        "    \"\"\"Generate high-quality audio using Edge TTS\"\"\"\n",
        "    if not text.strip():\n",
        "        text = \"‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§î‡§∞ ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à‡•§\"\n",
        "\n",
        "    print(f\"üéôÔ∏è Generating audio with voice: {SELECTED_VOICE}\")\n",
        "\n",
        "    # Configure speech parameters for better quality\n",
        "    communicate = edge_tts.Communicate(\n",
        "        text=text,\n",
        "        voice=SELECTED_VOICE,\n",
        "        rate=\"-10%\",  # Slightly slower for clarity\n",
        "        volume=\"+0%\",\n",
        "        pitch=\"+0Hz\"\n",
        "    )\n",
        "\n",
        "    await communicate.save(output_path)\n",
        "    print(f\"‚úÖ Audio saved: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# ================================\n",
        "# üé¨ PROFESSIONAL VIDEO CREATION\n",
        "# ================================\n",
        "import cv2\n",
        "import moviepy.editor as mp\n",
        "from PIL import Image\n",
        "\n",
        "# Define your constants somewhere in your script\n",
        "VIDEO_WIDTH = 1920\n",
        "VIDEO_HEIGHT = 1080\n",
        "FPS = 30\n",
        "BITRATE = \"5000k\"\n",
        "\n",
        "def create_professional_video(images, audio_file, output_file):\n",
        "    \"\"\"Create high-quality video with effects\"\"\"\n",
        "    print(\"üé¨ Creating professional video...\")\n",
        "\n",
        "    # Load audio\n",
        "    audio_clip = mp.AudioFileClip(audio_file)\n",
        "    total_duration = audio_clip.duration\n",
        "\n",
        "    # Calculate timing\n",
        "    num_images = len(images)\n",
        "    base_duration = total_duration / num_images\n",
        "\n",
        "    clips = []\n",
        "    for i, img_path in enumerate(images):\n",
        "        print(f\"üì∏ Processing image {i+1}/{num_images}\")\n",
        "\n",
        "        # ‚úÖ Force RGB to fix grayscale issues\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        safe_path = img_path.replace(\".jpg\", \"_rgb.jpg\").replace(\".png\", \"_rgb.png\")\n",
        "        img.save(safe_path)\n",
        "\n",
        "        # Create image clip\n",
        "        img_clip = mp.ImageClip(safe_path, duration=base_duration)\n",
        "\n",
        "        # Resize and pad to exact dimensions\n",
        "        img_clip = img_clip.resize(height=VIDEO_HEIGHT)\n",
        "        if img_clip.w > VIDEO_WIDTH:\n",
        "            img_clip = img_clip.resize(width=VIDEO_WIDTH)\n",
        "\n",
        "        # Center the image with black padding\n",
        "        img_clip = img_clip.on_color(\n",
        "            size=(VIDEO_WIDTH, VIDEO_HEIGHT),\n",
        "            color=(0, 0, 0),\n",
        "            pos=\"center\"\n",
        "        )\n",
        "\n",
        "        # Add subtle zoom effect\n",
        "        def zoom_effect(get_frame, t):\n",
        "            frame = get_frame(t)\n",
        "            zoom_factor = 1 + (t / base_duration) * 0.1  # 10% zoom over duration\n",
        "            h, w = frame.shape[:2]\n",
        "            new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n",
        "\n",
        "            if new_h > h and new_w > w:\n",
        "                frame = cv2.resize(frame, (new_w, new_h))\n",
        "                start_x = (new_w - w) // 2\n",
        "                start_y = (new_h - h) // 2\n",
        "                frame = frame[start_y:start_y+h, start_x:start_x+w]\n",
        "\n",
        "            return frame\n",
        "\n",
        "        img_clip = img_clip.fl(zoom_effect)\n",
        "\n",
        "        # Add fade transitions (except for first and last)\n",
        "        if i > 0:\n",
        "            img_clip = img_clip.fadein(0.5)\n",
        "        if i < num_images - 1:\n",
        "            img_clip = img_clip.fadeout(0.5)\n",
        "\n",
        "        clips.append(img_clip)\n",
        "\n",
        "    # Concatenate clips\n",
        "    final_video = mp.concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "    # Add audio\n",
        "    final_video = final_video.set_audio(audio_clip)\n",
        "\n",
        "    # Export with high quality settings\n",
        "    final_video.write_videofile(\n",
        "        output_file,\n",
        "        fps=FPS,\n",
        "        codec=\"libx264\",\n",
        "        audio_codec=\"aac\",\n",
        "        temp_audiofile=f\"{output_file}_temp_audio.m4a\",\n",
        "        remove_temp=True,\n",
        "        bitrate=BITRATE,\n",
        "        preset=\"medium\",  # Good balance of quality and encoding speed\n",
        "        ffmpeg_params=[\n",
        "            \"-crf\", \"18\",      # High quality (lower = better quality)\n",
        "            \"-profile:v\", \"high\",\n",
        "            \"-level\", \"4.0\",\n",
        "            \"-pix_fmt\", \"yuv420p\"  # YouTube compatibility\n",
        "        ],\n",
        "        threads=4\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Professional video created: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "# ================================\n",
        "# üöÄ MAIN EXECUTION\n",
        "# ================================\n",
        "async def main():\n",
        "    print(\"üé¨ AI Manga Recap Generator - Professional Edition\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Setup\n",
        "    device = setup_environment()\n",
        "\n",
        "    # Check if PDF exists\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        print(f\"‚ùå Please upload your PDF to: {PDF_PATH}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract images\n",
        "        print(\"\\nüìñ Step 1: Extracting PDF pages...\")\n",
        "        images = extract_pdf_images(PDF_PATH, max_pages=4)\n",
        "        print(f\"‚úÖ Extracted {len(images)} pages\")\n",
        "\n",
        "        # Step 2: OCR\n",
        "        print(\"\\nüîç Step 2: Advanced OCR processing...\")\n",
        "        raw_text = advanced_ocr(images)\n",
        "        print(f\"‚úÖ Extracted {len(raw_text)} characters of text\")\n",
        "\n",
        "        if len(raw_text) < 20:\n",
        "            print(\"‚ö†Ô∏è Limited text found. Using fallback content.\")\n",
        "\n",
        "        # Step 3: Generate content\n",
        "        print(\"\\nü§ñ Step 3: AI content generation...\")\n",
        "        generator = ContentGenerator(device)\n",
        "        hindi_script = generator.generate_engaging_script(raw_text)\n",
        "        print(f\"‚úÖ Generated script: {len(hindi_script)} characters\")\n",
        "        print(f\"Preview: {hindi_script[:100]}...\")\n",
        "\n",
        "        # Step 4: Create audio\n",
        "        print(\"\\nüéôÔ∏è Step 4: High-quality TTS generation...\")\n",
        "        audio_path = os.path.join(OUTPUT_DIR, \"narration.wav\")\n",
        "        await create_high_quality_audio(hindi_script, audio_path)\n",
        "\n",
        "        # Step 5: Create video\n",
        "        print(\"\\nüé¨ Step 5: Professional video creation...\")\n",
        "        video_path = os.path.join(OUTPUT_DIR, f\"manga_recap_chapter_{CHAPTER}.mp4\")\n",
        "        create_professional_video(images, audio_path, video_path)\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üéâ SUCCESS! Video generation complete!\")\n",
        "        print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
        "        print(f\"üé• Video file: {video_path}\")\n",
        "        print(f\"üìù Script file: {os.path.join(OUTPUT_DIR, 'generated_script.txt')}\")\n",
        "        print(f\"üîä Audio file: {audio_path}\")\n",
        "\n",
        "        # File size info\n",
        "        if os.path.exists(video_path):\n",
        "            size_mb = os.path.getsize(video_path) / (1024 * 1024)\n",
        "            print(f\"üìä Video size: {size_mb:.2f} MB\")\n",
        "\n",
        "        print(\"üöÄ Ready for YouTube upload!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ================================\n",
        "# üèÉ RUN\n",
        "# ================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the async main function\n",
        "    asyncio.run(main())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé¨ AI Manga Recap Generator - Professional Edition\n",
            "============================================================\n",
            "üöÄ Using device: cuda\n",
            "\n",
            "üìñ Step 1: Extracting PDF pages...\n",
            "‚è© Skipped extraction, using cached: page 1\n",
            "‚è© Skipped extraction, using cached: page 2\n",
            "‚è© Skipped extraction, using cached: page 3\n",
            "‚è© Skipped extraction, using cached: page 4\n",
            "‚úÖ Extracted 4 pages\n",
            "\n",
            "üîç Step 2: Advanced OCR processing...\n",
            "üîé Processing OCR for page 1...\n",
            "‚ö†Ô∏è Resized page 1 for OCR: 2316x43197 ‚Üí 107x2000\n",
            "üìÑ Page 1: 5 characters extracted\n",
            "üîé Processing OCR for page 2...\n",
            "‚ö†Ô∏è Resized page 2 for OCR: 2400x42645 ‚Üí 112x2000\n",
            "üìÑ Page 2: 12 characters extracted\n",
            "üîé Processing OCR for page 3...\n",
            "‚ö†Ô∏è Resized page 3 for OCR: 2400x15801 ‚Üí 303x2000\n",
            "üìÑ Page 3: 0 characters extracted\n",
            "üîé Processing OCR for page 4...\n",
            "‚ö†Ô∏è Resized page 4 for OCR: 2304x43197 ‚Üí 106x2000\n",
            "üìÑ Page 4: 10 characters extracted\n",
            "‚úÖ Extracted 30 characters of text\n",
            "\n",
            "ü§ñ Step 3: AI content generation...\n",
            "ü§ñ Loading AI models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ AI models loaded successfully!\n",
            "‚úÖ Generated script: 57 characters\n",
            "Preview: ‡§á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§™‡§∏‡§Ç‡§¶ ‡§Ü‡§è‡§ó‡•Ä‡•§...\n",
            "\n",
            "üéôÔ∏è Step 4: High-quality TTS generation...\n",
            "üéôÔ∏è Generating audio with voice: hi-IN-MadhurNeural\n",
            "‚úÖ Audio saved: output/narration.wav\n",
            "\n",
            "üé¨ Step 5: Professional video creation...\n",
            "üé¨ Creating professional video...\n",
            "üì∏ Processing image 1/4\n",
            "üì∏ Processing image 2/4\n",
            "üì∏ Processing image 3/4\n",
            "üì∏ Processing image 4/4\n",
            "Moviepy - Building video output/manga_recap_chapter_1.mp4.\n",
            "MoviePy - Writing audio in output/manga_recap_chapter_1.mp4_temp_audio.m4a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video output/manga_recap_chapter_1.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready output/manga_recap_chapter_1.mp4\n",
            "‚úÖ Professional video created: output/manga_recap_chapter_1.mp4\n",
            "\n",
            "============================================================\n",
            "üéâ SUCCESS! Video generation complete!\n",
            "üìÅ Output directory: output\n",
            "üé• Video file: output/manga_recap_chapter_1.mp4\n",
            "üìù Script file: output/generated_script.txt\n",
            "üîä Audio file: output/narration.wav\n",
            "üìä Video size: 0.46 MB\n",
            "üöÄ Ready for YouTube upload!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-y2biJa5aNUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KE0eH9KVaNYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Enhanced AI Manga Recap Generator (Colab-Optimized for T4 GPU)\n",
        "- Extracts high-quality pages from PDF\n",
        "- Advanced OCR with noise reduction\n",
        "- Better AI models for content generation\n",
        "- High-quality TTS with multiple voices\n",
        "- Professional video generation with effects\n",
        "\"\"\"\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Install dependencies (run this cell first)\n",
        "# ================================\n",
        "\"\"\"\n",
        "!pip install -q moviepy pillow easyocr PyMuPDF transformers torch accelerate\n",
        "!pip install -q edge-tts asyncio nest-asyncio\n",
        "!pip install -q opencv-python numpy scikit-image\n",
        "!pip install -q sentence-transformers\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import fitz  # PyMuPDF\n",
        "import easyocr\n",
        "import edge_tts\n",
        "import moviepy.editor as mp\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Enable nested asyncio for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ================================\n",
        "# üéØ ENHANCED CONFIG\n",
        "# ================================\n",
        "PDF_PATH = \"/content/Ch_199_Side_Story_20.pdf\"   # Upload your PDF in Colab\n",
        "OUTPUT_DIR = \"output\"\n",
        "CHAPTER = 1\n",
        "\n",
        "# AI Models (optimized for T4)\n",
        "SUMMARIZATION_MODEL = \"facebook/bart-large-cnn\"  # Better summarization\n",
        "TRANSLATION_MODEL = \"Helsinki-NLP/opus-mt-en-hi\"  # English to Hindi\n",
        "\n",
        "# Voice settings (Edge TTS - high quality, free)\n",
        "VOICE_OPTIONS = [\n",
        "    \"hi-IN-MadhurNeural\",     # Male, clear\n",
        "    \"hi-IN-SwaraNeural\",      # Female, expressive\n",
        "    \"hi-IN-AnanyaNeural\"      # Female, warm\n",
        "]\n",
        "SELECTED_VOICE = VOICE_OPTIONS[0]  # Change index for different voice\n",
        "\n",
        "# Video settings\n",
        "VIDEO_WIDTH = 1920\n",
        "VIDEO_HEIGHT = 1080\n",
        "FPS = 30\n",
        "BITRATE = \"8000k\"  # High quality for YouTube\n",
        "\n",
        "# ================================\n",
        "# üîß UTILITY FUNCTIONS\n",
        "# ================================\n",
        "def setup_environment():\n",
        "    \"\"\"Setup GPU and create directories\"\"\"\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"üöÄ Using device: {device}\")\n",
        "    return device\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    \"\"\"Enhance image quality for better OCR\"\"\"\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Denoise\n",
        "    denoised = cv2.fastNlMeansDenoising(gray)\n",
        "\n",
        "    # Enhance contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    enhanced = clahe.apply(denoised)\n",
        "\n",
        "    # Sharpen\n",
        "    kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
        "    sharpened = cv2.filter2D(enhanced, -1, kernel)\n",
        "\n",
        "    # Save enhanced image\n",
        "    enhanced_path = img_path.replace('.jpg', '_enhanced.jpg')\n",
        "    cv2.imwrite(enhanced_path, sharpened)\n",
        "    return enhanced_path\n",
        "\n",
        "# ================================\n",
        "# üìñ PDF EXTRACTION (ENHANCED)\n",
        "# ================================\n",
        "def extract_pdf_images(pdf_path, max_pages=5):\n",
        "    \"\"\"Extract high-quality images from PDF\"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    images = []\n",
        "\n",
        "    for i in range(min(len(doc), max_pages)):\n",
        "        page = doc.load_page(i)\n",
        "\n",
        "        # High-resolution matrix for better quality\n",
        "        mat = fitz.Matrix(3.0, 3.0)  # 3x zoom for crisp images\n",
        "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "\n",
        "        img_path = os.path.join(OUTPUT_DIR, f\"page_{i+1}.jpg\")\n",
        "        pix.save(img_path)\n",
        "\n",
        "        # Enhance image for OCR\n",
        "        enhanced_path = preprocess_image(img_path)\n",
        "        images.append(enhanced_path)\n",
        "\n",
        "        print(f\"‚úÖ Extracted page {i+1}\")\n",
        "\n",
        "    doc.close()\n",
        "    return images\n",
        "\n",
        "# ================================\n",
        "# üîç ADVANCED OCR\n",
        "# ================================\n",
        "def advanced_ocr(images):\n",
        "    \"\"\"Enhanced OCR with text cleaning\"\"\"\n",
        "    reader = easyocr.Reader(['en', 'hi'], gpu=torch.cuda.is_available())\n",
        "    all_text = []\n",
        "\n",
        "    for idx, img in enumerate(images):\n",
        "        print(f\"üîé Processing OCR for page {idx+1}...\")\n",
        "\n",
        "        # Get OCR results with confidence scores\n",
        "        results = reader.readtext(img, detail=1, paragraph=True)\n",
        "\n",
        "        # Filter by confidence threshold\n",
        "        filtered_text = []\n",
        "        for (bbox, text, confidence) in results:\n",
        "            if confidence > 0.5:  # Only high-confidence text\n",
        "                # Clean text\n",
        "                clean_text = re.sub(r'[^\\w\\s\\u0900-\\u097F]', ' ', text)\n",
        "                clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "                if len(clean_text) > 2:  # Skip very short text\n",
        "                    filtered_text.append(clean_text)\n",
        "\n",
        "        page_text = \" \".join(filtered_text)\n",
        "        all_text.append(page_text)\n",
        "        print(f\"üìÑ Page {idx+1}: {len(page_text)} characters extracted\")\n",
        "\n",
        "    combined_text = \" \".join(all_text)\n",
        "\n",
        "    # Save extracted text for debugging\n",
        "    with open(os.path.join(OUTPUT_DIR, \"extracted_text.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(combined_text)\n",
        "\n",
        "    return combined_text\n",
        "\n",
        "# ================================\n",
        "# ü§ñ ENHANCED AI CONTENT GENERATION\n",
        "# ================================\n",
        "class ContentGenerator:\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        print(\"ü§ñ Loading AI models...\")\n",
        "\n",
        "        # Summarization model\n",
        "        self.summarizer = pipeline(\n",
        "            \"summarization\",\n",
        "            model=SUMMARIZATION_MODEL,\n",
        "            device=0 if device == \"cuda\" else -1,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "        )\n",
        "\n",
        "        # Translation model\n",
        "        self.translator_tokenizer = AutoTokenizer.from_pretrained(TRANSLATION_MODEL)\n",
        "        self.translator_model = AutoModelForSeq2SeqLM.from_pretrained(TRANSLATION_MODEL)\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            self.translator_model = self.translator_model.half().cuda()\n",
        "\n",
        "        print(\"‚úÖ AI models loaded successfully!\")\n",
        "\n",
        "    def generate_engaging_script(self, raw_text):\n",
        "        \"\"\"Generate engaging Hindi narration\"\"\"\n",
        "        if not raw_text or len(raw_text) < 50:\n",
        "            return \"‡§á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§π‡•à ‡§ú‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§™‡§∏‡§Ç‡§¶ ‡§Ü‡§è‡§ó‡•Ä‡•§\"\n",
        "\n",
        "        try:\n",
        "            # Step 1: Summarize in English (better quality)\n",
        "            print(\"üìù Generating summary...\")\n",
        "\n",
        "            # Split text if too long\n",
        "            max_chunk = 1000\n",
        "            if len(raw_text) > max_chunk:\n",
        "                chunks = [raw_text[i:i+max_chunk] for i in range(0, len(raw_text), max_chunk)]\n",
        "                summaries = []\n",
        "                for chunk in chunks:\n",
        "                    if len(chunk) > 50:\n",
        "                        summary = self.summarizer(\n",
        "                            chunk,\n",
        "                            max_length=100,\n",
        "                            min_length=30,\n",
        "                            do_sample=False\n",
        "                        )[0]['summary_text']\n",
        "                        summaries.append(summary)\n",
        "                english_summary = \" \".join(summaries)\n",
        "            else:\n",
        "                english_summary = self.summarizer(\n",
        "                    raw_text,\n",
        "                    max_length=150,\n",
        "                    min_length=40,\n",
        "                    do_sample=False\n",
        "                )[0]['summary_text']\n",
        "\n",
        "            # Step 2: Enhance for storytelling\n",
        "            storytelling_prompt = f\"\"\"\n",
        "            Transform this manga summary into an engaging Hindi narration for YouTube:\n",
        "            {english_summary}\n",
        "\n",
        "            Make it dramatic and exciting for viewers.\n",
        "            \"\"\"\n",
        "\n",
        "            # Step 3: Translate to Hindi\n",
        "            print(\"üåç Translating to Hindi...\")\n",
        "            inputs = self.translator_tokenizer(\n",
        "                english_summary,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "            if self.device == \"cuda\":\n",
        "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.translator_model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=200,\n",
        "                    num_beams=4,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True\n",
        "                )\n",
        "\n",
        "            hindi_text = self.translator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Step 4: Add engaging elements\n",
        "            hindi_text = self.enhance_narration(hindi_text)\n",
        "\n",
        "            # Save generated script\n",
        "            with open(os.path.join(OUTPUT_DIR, \"generated_script.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"English Summary:\\n{english_summary}\\n\\nHindi Script:\\n{hindi_text}\")\n",
        "\n",
        "            return hindi_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è AI generation failed: {e}\")\n",
        "            return \"‡§á‡§∏ ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§ò‡§ü‡§®‡§æ‡§è‡§Ç ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§Ü‡§™‡§ï‡•ã ‡§π‡•à‡§∞‡§æ‡§® ‡§ï‡§∞ ‡§¶‡•á‡§Ç‡§ó‡•Ä‡•§ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§®‡§è ‡§Æ‡•ã‡§°‡§º ‡§Ü‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§ï‡§ø‡§∞‡§¶‡§æ‡§∞ ‡§Ö‡§™‡§®‡•Ä ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ ‡§Æ‡•á‡§Ç ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡§§‡•á ‡§π‡•à‡§Ç‡•§\"\n",
        "\n",
        "    def enhance_narration(self, text):\n",
        "        \"\"\"Add dramatic elements to Hindi narration\"\"\"\n",
        "        # Add pauses and emphasis\n",
        "        text = text.replace(\"‡•§\", \"... \")\n",
        "        text = text.replace(\",\", \"... \")\n",
        "\n",
        "        # Add engaging intro/outro\n",
        "        intros = [\n",
        "            \"‡§Ü‡§ú ‡§ï‡•á ‡§á‡§∏ ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§Æ‡•á‡§Ç... \",\n",
        "            \"‡§á‡§∏ ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§ï‡§π‡§æ‡§®‡•Ä ‡§Æ‡•á‡§Ç ‡§Ü‡§ó‡•á... \",\n",
        "            \"‡§Ö‡§¨ ‡§¶‡•á‡§ñ‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à... \"\n",
        "        ]\n",
        "\n",
        "        outros = [\n",
        "            \"... ‡§Ø‡§π ‡§ï‡§π‡§æ‡§®‡•Ä ‡§ï‡§ø‡§§‡§®‡•Ä ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à!\",\n",
        "            \"... ‡§Ü‡§ó‡•á ‡§î‡§∞ ‡§≠‡•Ä ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö ‡§ï‡§æ ‡§á‡§Ç‡§§‡§ú‡§æ‡§∞ ‡§π‡•à!\",\n",
        "            \"... ‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§µ‡§æ‡§ï‡§à ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§•‡§æ!\"\n",
        "        ]\n",
        "\n",
        "        enhanced = f\"{intros[0]}{text}{outros[0]}\"\n",
        "        return enhanced\n",
        "\n",
        "# ================================\n",
        "# üéôÔ∏è HIGH-QUALITY TTS\n",
        "# ================================\n",
        "async def create_high_quality_audio(text, output_path):\n",
        "    \"\"\"Generate high-quality audio using Edge TTS\"\"\"\n",
        "    if not text.strip():\n",
        "        text = \"‡§Ø‡§π ‡§Ö‡§ß‡•ç‡§Ø‡§æ‡§Ø ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§∞‡•ã‡§Æ‡§æ‡§Ç‡§ö‡§ï ‡§î‡§∞ ‡§¶‡§ø‡§≤‡§ö‡§∏‡•ç‡§™ ‡§π‡•à‡•§\"\n",
        "\n",
        "    print(f\"üéôÔ∏è Generating audio with voice: {SELECTED_VOICE}\")\n",
        "\n",
        "    # Configure speech parameters for better quality\n",
        "    communicate = edge_tts.Communicate(\n",
        "        text=text,\n",
        "        voice=SELECTED_VOICE,\n",
        "        rate=\"-10%\",  # Slightly slower for clarity\n",
        "        volume=\"+0%\",\n",
        "        pitch=\"+0Hz\"\n",
        "    )\n",
        "\n",
        "    await communicate.save(output_path)\n",
        "    print(f\"‚úÖ Audio saved: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# ================================\n",
        "# üé¨ PROFESSIONAL VIDEO CREATION\n",
        "# ================================\n",
        "def create_professional_video(images, audio_file, output_file):\n",
        "    \"\"\"Create high-quality video with effects\"\"\"\n",
        "    print(\"üé¨ Creating professional video...\")\n",
        "\n",
        "    # Load audio\n",
        "    audio_clip = mp.AudioFileClip(audio_file)\n",
        "    total_duration = audio_clip.duration\n",
        "\n",
        "    # Calculate timing\n",
        "    num_images = len(images)\n",
        "    base_duration = total_duration / num_images\n",
        "\n",
        "    clips = []\n",
        "    for i, img_path in enumerate(images):\n",
        "        print(f\"üì∏ Processing image {i+1}/{num_images}\")\n",
        "\n",
        "        # Create image clip\n",
        "        img_clip = mp.ImageClip(img_path, duration=base_duration)\n",
        "\n",
        "        # Resize and pad to exact dimensions\n",
        "        img_clip = img_clip.resize(height=VIDEO_HEIGHT)\n",
        "\n",
        "        if img_clip.w > VIDEO_WIDTH:\n",
        "            img_clip = img_clip.resize(width=VIDEO_WIDTH)\n",
        "\n",
        "        # Center the image with black padding\n",
        "        img_clip = img_clip.on_color(\n",
        "            size=(VIDEO_WIDTH, VIDEO_HEIGHT),\n",
        "            color=(0, 0, 0),\n",
        "            pos='center'\n",
        "        )\n",
        "\n",
        "        # Add subtle zoom effect\n",
        "        def zoom_effect(get_frame, t):\n",
        "            frame = get_frame(t)\n",
        "            zoom_factor = 1 + (t / base_duration) * 0.1  # 10% zoom over duration\n",
        "            h, w = frame.shape[:2]\n",
        "            new_h, new_w = int(h * zoom_factor), int(w * zoom_factor)\n",
        "\n",
        "            if new_h > h and new_w > w:\n",
        "                frame = cv2.resize(frame, (new_w, new_h))\n",
        "                start_x = (new_w - w) // 2\n",
        "                start_y = (new_h - h) // 2\n",
        "                frame = frame[start_y:start_y+h, start_x:start_x+w]\n",
        "\n",
        "            return frame\n",
        "\n",
        "        img_clip = img_clip.fl(zoom_effect)\n",
        "\n",
        "        # Add fade transitions (except for first and last)\n",
        "        if i > 0:\n",
        "            img_clip = img_clip.fadein(0.5)\n",
        "        if i < num_images - 1:\n",
        "            img_clip = img_clip.fadeout(0.5)\n",
        "\n",
        "        clips.append(img_clip)\n",
        "\n",
        "    # Concatenate clips\n",
        "    final_video = mp.concatenate_videoclips(clips, method=\"compose\")\n",
        "\n",
        "    # Add audio\n",
        "    final_video = final_video.set_audio(audio_clip)\n",
        "\n",
        "    # Export with high quality settings\n",
        "    final_video.write_videofile(\n",
        "        output_file,\n",
        "        fps=FPS,\n",
        "        codec='libx264',\n",
        "        audio_codec='aac',\n",
        "        temp_audiofile=f\"{output_file}_temp_audio.m4a\",\n",
        "        remove_temp=True,\n",
        "        bitrate=BITRATE,\n",
        "        preset='medium',  # Good balance of quality and encoding speed\n",
        "        ffmpeg_params=[\n",
        "            '-crf', '18',  # High quality (lower = better quality)\n",
        "            '-profile:v', 'high',\n",
        "            '-level', '4.0',\n",
        "            '-pix_fmt', 'yuv420p'  # YouTube compatibility\n",
        "        ],\n",
        "        threads=4\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Professional video created: {output_file}\")\n",
        "    return output_file\n",
        "\n",
        "# ================================\n",
        "# üöÄ MAIN EXECUTION\n",
        "# ================================\n",
        "async def main():\n",
        "    print(\"üé¨ AI Manga Recap Generator - Professional Edition\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Setup\n",
        "    device = setup_environment()\n",
        "\n",
        "    # Check if PDF exists\n",
        "    if not os.path.exists(PDF_PATH):\n",
        "        print(f\"‚ùå Please upload your PDF to: {PDF_PATH}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Step 1: Extract images\n",
        "        print(\"\\nüìñ Step 1: Extracting PDF pages...\")\n",
        "        images = extract_pdf_images(PDF_PATH, max_pages=6)  # More pages for longer content\n",
        "        print(f\"‚úÖ Extracted {len(images)} pages\")\n",
        "\n",
        "        # Step 2: OCR\n",
        "        print(\"\\nüîç Step 2: Advanced OCR processing...\")\n",
        "        raw_text = advanced_ocr(images)\n",
        "        print(f\"‚úÖ Extracted {len(raw_text)} characters of text\")\n",
        "\n",
        "        if len(raw_text) < 20:\n",
        "            print(\"‚ö†Ô∏è Limited text found. Using fallback content.\")\n",
        "\n",
        "        # Step 3: Generate content\n",
        "        print(\"\\nü§ñ Step 3: AI content generation...\")\n",
        "        generator = ContentGenerator(device)\n",
        "        hindi_script = generator.generate_engaging_script(raw_text)\n",
        "        print(f\"‚úÖ Generated script: {len(hindi_script)} characters\")\n",
        "        print(f\"Preview: {hindi_script[:100]}...\")\n",
        "\n",
        "        # Step 4: Create audio\n",
        "        print(\"\\nüéôÔ∏è Step 4: High-quality TTS generation...\")\n",
        "        audio_path = os.path.join(OUTPUT_DIR, \"narration.wav\")\n",
        "        await create_high_quality_audio(hindi_script, audio_path)\n",
        "\n",
        "        # Step 5: Create video\n",
        "        print(\"\\nüé¨ Step 5: Professional video creation...\")\n",
        "        video_path = os.path.join(OUTPUT_DIR, f\"manga_recap_chapter_{CHAPTER}.mp4\")\n",
        "        create_professional_video(images, audio_path, video_path)\n",
        "\n",
        "        # Summary\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üéâ SUCCESS! Video generation complete!\")\n",
        "        print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
        "        print(f\"üé• Video file: {video_path}\")\n",
        "        print(f\"üìù Script file: {os.path.join(OUTPUT_DIR, 'generated_script.txt')}\")\n",
        "        print(f\"üîä Audio file: {audio_path}\")\n",
        "\n",
        "        # File size info\n",
        "        if os.path.exists(video_path):\n",
        "            size_mb = os.path.getsize(video_path) / (1024 * 1024)\n",
        "            print(f\"üìä Video size: {size_mb:.2f} MB\")\n",
        "\n",
        "        print(\"üöÄ Ready for YouTube upload!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ================================\n",
        "# üèÉ RUN\n",
        "# ================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the async main function\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Ua6xBetMR6",
        "outputId": "9b7c67b4-7107-4f36-bf80-b6e6a8594601"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé¨ AI Manga Recap Generator - Professional Edition\n",
            "============================================================\n",
            "üöÄ Using device: cuda\n",
            "‚ùå Please upload your PDF to: /content/Ch_199_Side_Story_20.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLvYMvxgtl-q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}